# Validation Phase Documentation

This folder contains all files and scripts related to the **validation phase** of the SEALL project, as described in our ASE 2025 NIER Track submission. The purpose of this phase is to evaluate the effectiveness of the SEALL app in generating accurate and useful search strings for systematic literature reviews (SLRs).

## Purpose of Validation

The goal of this validation is to compare the **search strings generated by the SEALL app** with the **final search strings used by researchers** in published systematic reviews. To quantify this comparison, we use the **Jaccard similarity index**, which measures the overlap of words between two sets of text.

This process helps assess how well the SEALL app performs in replicating the search strategies designed by human experts.

## Folder Structure and Key Files

### `dataset/`

This folder contains the **original, unmodified dataset** used in the validation phase. It consists of the raw data collected before any processing or enrichment.

### `final_dataset-June-2023-modified.xlsx`

This file is an enriched version of the original dataset. The following information was manually added:
- Research questions
- Final search strings

These were obtained directly from the papers referenced in the dataset.

## Validation Workflow

The validation process consists of a sequence of scripts designed to process the dataset, generate AI-based search strings using the SEALL backend, and compare them to the original human-authored search strings.

### 1. `finegrainselector.py`

This script filters entries from `final_dataset-June-2023-modified.xlsx` that meet specific conditions, such as the presence of both a research question and a search string.

**Output:** `final_paper_selection.xlsx`

### 2. `extract_specific_cols_toNewFile.py`

This script extracts only the relevant columns from the filtered data. These include:
- Paper title
- Research question
- Final search string
- Abstract (if available)

**Output:** `filtered_output.xlsx`

### 3. `generate_bot_search_strings.py`

This script generates search strings using the SEALL backend.

**Important:** Ensure that the **Flask backend application is running** before executing this script.

The script:
- Iterates through each row in `filtered_output.xlsx`
- Sends a prompt with context (e.g., research question) to the backend
- Receives the generated search string
- Saves the AI-generated string back into the same file

**Input/Output:** `filtered_output.xlsx` (modified in place)

### 4. `validation.py`

This script compares the AI-generated search strings to the human-written ones using the **Jaccard similarity index**.

**Jaccard Index Formula:**

```
J(A, B) = |A ∩ B| / |A ∪ B|
```

Where:
- `A` = set of words from the AI-generated string
- `B` = set of words from the final search string in the study

Each comparison produces a numeric score representing the degree of similarity between the two search strings.

## Summary

This validation pipeline enables a systematic evaluation of the SEALL app’s ability to generate meaningful and accurate search strings. The process demonstrates how closely the app can approximate the work of expert researchers when it comes to designing systematic search strategies.

## Additional Notes

- Run the scripts in the exact order described above.
- The backend server must be active before executing `generate_bot_search_strings.py`.
- All intermediate outputs are saved step-by-step and reused in subsequent stages.

This workflow was designed to validate the SEALL application's capabilities and was a critical part of the ASE 2025 submission.